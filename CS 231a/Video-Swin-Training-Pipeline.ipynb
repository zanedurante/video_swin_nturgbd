{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5dba629",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from mmaction.apis import init_recognizer, inference_recognizer, train_model\n",
    "from mmaction.datasets import RawframeDataset\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee768f8",
   "metadata": {},
   "source": [
    "## Construct Training Dataset and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b21466",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/vision/group/ntu-rgbd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d1087",
   "metadata": {},
   "source": [
    "### Setup data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04ebd429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using training pipeline from the Swin Tiny Config file set above\n",
    "train_ann_filename = DATASET_PATH + 'rgb_train_ann.txt'\n",
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
    "train_pipeline = [\n",
    "    dict(type='DecordInit'),\n",
    "    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n",
    "    dict(type='DecordDecode'),\n",
    "    dict(type='Resize', scale=(-1, 256)),\n",
    "    dict(type='RandomResizedCrop'),\n",
    "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
    "    dict(type='Flip', flip_ratio=0.5),\n",
    "    dict(type='Normalize', **img_norm_cfg),\n",
    "    dict(type='FormatShape', input_format='NCTHW'),\n",
    "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
    "]\n",
    "\n",
    "val_pipeline = [\n",
    "    dict(type='DecordInit'),\n",
    "    dict(\n",
    "        type='SampleFrames',\n",
    "        clip_len=32,\n",
    "        frame_interval=2,\n",
    "        num_clips=1,\n",
    "        test_mode=True),\n",
    "    dict(type='DecordDecode'),\n",
    "    dict(type='Resize', scale=(-1, 256)),\n",
    "    dict(type='CenterCrop', crop_size=224),\n",
    "    dict(type='Flip', flip_ratio=0),\n",
    "    dict(type='Normalize', **img_norm_cfg),\n",
    "    dict(type='FormatShape', input_format='NCTHW'),\n",
    "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "    dict(type='ToTensor', keys=['imgs'])\n",
    "]\n",
    "\n",
    "test_pipeline = [\n",
    "    dict(type='DecordInit'),\n",
    "    dict(\n",
    "        type='SampleFrames',\n",
    "        clip_len=32,\n",
    "        frame_interval=2,\n",
    "        num_clips=4,\n",
    "        test_mode=True),\n",
    "    dict(type='DecordDecode'),\n",
    "    dict(type='Resize', scale=(-1, 224)),\n",
    "    dict(type='ThreeCrop', crop_size=224),\n",
    "    dict(type='Flip', flip_ratio=0),\n",
    "    dict(type='Normalize', **img_norm_cfg),\n",
    "    dict(type='FormatShape', input_format='NCTHW'),\n",
    "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "    dict(type='ToTensor', keys=['imgs'])\n",
    "]\n",
    "num_classes = 60 # Need to change to 120 for NTU RGB-D 120\n",
    "train_dataset = RawframeDataset(train_ann_filename, train_pipeline, num_classes=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f86e579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'RawframeDataset'\n",
    "ann_file_train = DATASET_PATH + 'rgb_train_ann.txt'\n",
    "ann_file_val = DATASET_PATH + 'rgb_valid_ann.txt'\n",
    "ann_file_test = DATASET_PATH + 'rgb_test_ann.txt'\n",
    "\n",
    "data = dict(\n",
    "    videos_per_gpu=8,\n",
    "    workers_per_gpu=4,\n",
    "    val_dataloader=dict(\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1\n",
    "    ),\n",
    "    test_dataloader=dict(\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=1\n",
    "    ),\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        ann_file=ann_file_train,\n",
    "        pipeline=train_pipeline),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        ann_file=ann_file_val,\n",
    "        pipeline=val_pipeline),\n",
    "    test=dict(\n",
    "        type=dataset_type,\n",
    "        ann_file=ann_file_test,\n",
    "        pipeline=test_pipeline))\n",
    "evaluation = dict(\n",
    "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96491f11",
   "metadata": {},
   "source": [
    "## Load pre-trained weights and change head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aacd37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ../models/pre-trained/swin/swin_tiny_patch244_window877_kinetics400_1k.pth\n"
     ]
    }
   ],
   "source": [
    "# Load Tiny Video-Swin Transformer pre-trained on Kinetics\n",
    "config_file = '../Video-Swin-Transformer/configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py'\n",
    "checkpoint_file = '../models/pre-trained/swin/swin_tiny_patch244_window877_kinetics400_1k.pth'\n",
    "device = 'cpu' #'cuda:0' # or 'cpu' # CHANGE WHEN USING GPU/CPU\n",
    "device = torch.device(device)\n",
    "model = init_recognizer(config_file, checkpoint_file, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d46a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the head of the model\n",
    "# Uses He initialization, could be something to look into: \n",
    "# https://arxiv.org/pdf/2002.06305.pdf investigates how different seeds and data ordering\n",
    "# strongly changes performance of the trained model\n",
    "model.cls_head.fc_cls = torch.nn.Linear(in_features=768, out_features=num_classes, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100bbab9",
   "metadata": {},
   "source": [
    "## Freeze model weights (except for head)\n",
    "This tutorial talks about fine-tuning: https://mmaction2.readthedocs.io/en/latest/tutorials/2_finetune.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f40a6b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.cls_head.fc_cls.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fcd6e",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5aa145f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'log_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Video-Swin-Transformer/mmaction/apis/train.py:45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, cfg, distributed, validate, test, timestamp, meta)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model,\n\u001b[1;32m     21\u001b[0m                 dataset,\n\u001b[1;32m     22\u001b[0m                 cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m                 meta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"Train model entry function.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m            Default: None\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     logger \u001b[38;5;241m=\u001b[39m get_root_logger(log_level\u001b[38;5;241m=\u001b[39m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_level\u001b[49m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# prepare data loaders\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m [dataset]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'log_level'"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataset, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c06332",
   "metadata": {},
   "source": [
    "## Model training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efae83b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
